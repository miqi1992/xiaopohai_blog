---
doc_type: weread-highlights-reviews
bookId: "860101"
author: 范传辉
cover: https://wfqqreader-1252317822.image.myqcloud.com/cover/101/860101/t7_860101.jpg
reviewCount: 1
noteCount: 7
readingStatus: 读完
progress: 100%
totalReadDay: 13
readingTime: 4小时35分钟
readingDate: 2017-12-25
finishedDate: 2018-01-02
isbn: 9787111563877
lastReadDate: 2018-12-17

---
# 元数据
> [!abstract] Python爬虫开发与项目实战
> - ![ Python爬虫开发与项目实战|200](https://wfqqreader-1252317822.image.myqcloud.com/cover/101/860101/t7_860101.jpg)
> - 书名： Python爬虫开发与项目实战
> - 作者： 范传辉
> - 简介： 本书总体来说是一本实战型书籍，以大量系统的实战项目为驱动，由浅及深地讲解了爬虫开发中所需的知识和技能。本书是一本适合初学者的书籍，既有对基础知识点的讲解，也涉及关键问题和难点的分析和解决，本书的初衷是帮助初学者夯实基础，实现提高。还有一点要说明，这本书对编程能力是有一定要求的，希望读者尽量熟悉Python 编程。
> - 出版时间 2017-06-01 00:00:00
> - ISBN： 9787111563877
> - 分类： 计算机-编程设计
> - 出版社： 机械工业出版社
> - PC地址：https://weread.qq.com/web/reader/aac325705d1fc5aacf4cbb9

# 高亮划线

## 封面

## 版权信息

## 前言

## 基础篇

### 第1章 回顾Python编程

> 📌 open函数中第三个可选参数buffering控制着文件的缓冲。如果参数是0, I/O操作就是无缓冲的，直接将数据写到硬盘上；如果参数是1, I/O操作就是有缓冲的，数据先写到内存里，只有使用flush函数或者close函数才会将数据更新到硬盘；如果参数为大于1的数字则代表缓冲区的大小（单位是字节）, -1（或者是任何负数）代表使用默认缓冲区的大小。 
> ⏱ 2017-12-31 10:09:24 ^860101-5-10644-10853

> 📌 调用read()一次将文件内容读到内存，但是如果文件过大，将会出现内存不足的问题。一般对于大文件，可以反复调用read(size)方法，一次最多读取size个字节。如果文件是文本文件，Python提供了更加合理的做法，调用readline()可以每次读取一行内容，调用readlines()一次读取所有内容并按行返回列表。 
> ⏱ 2017-12-31 10:10:51 ^860101-5-11720-11881

> 📌 Python中提供了两个模块：cPickle和pickle来实现序列化，前者是由C语言编写的，效率比后者高很多，但是两个模块的功能是一样的。一般编写程序的时候，采取的方案是先导入cPickle模块，如果此模块不存在，再导入pickle模块。示例如下：
￼ try:￼     import cPickle as pickle￼ except ImportError:￼     import pickle 
> ⏱ 2017-12-31 10:28:52 ^860101-5-15558-15805

### 第2章 Web前端基础

### 第3章 初识网络爬虫

### 第4章 HTML解析大法

### 第5章 数据存储（无数据库版）

### 第6章 实战项目：基础爬虫

### 第7章 实战项目：简单分布式爬虫

## 中级篇

### 第8章 数据存储（数据库版）

### 第9章 动态网站抓取

### 第10章 Web端协议分析

### 第11章 终端协议分析

### 第12章 初窥Scrapy爬虫框架

> 📌 提供了类似于词典的API以及用于声明可用字段的简单语法。Item使用简单的class定义语法以及Field对象来声明。在新建的cnblogSpider项目中，有一个items.py文件，用来定义存储数 
> ⏱ 2017-12-28 22:28:53 ^860101-17-16677

> 📌 我们需要继承FilesPipeline或者ImagesPipeline，重写get_media_requests和item_completed()方法。 
> ⏱ 2018-01-02 20:44:39 ^860101-17-31989-32065

### 第13章 深入Scrapy爬虫框架

> 📌 Request对象代表着一个HTTP请求，通常在Spider类中产生，然后传递给下载器，最后返回一个响应。 
> ⏱ 2017-12-29 21:11:29 ^860101-18-23982-24064

> 📌 每个Request通过下载中间件时，该方法被调用，返回值必须为None、Response对象、Request对象中的一个或Raise IgnoreRequest异常。 
> ⏱ 2017-12-28 22:32:38 ^860101-18-34113-34229

### 第14章 实战项目：Scrapy爬虫

## 深入篇

### 第15章 增量式爬虫

### 第16章 分布式爬虫与Scrapy

> 📌 表16-1 配置参数
￼ 
> ⏱ 2018-01-01 14:50:30 ^860101-22-4671-4811

### 第17章 实战项目：Scrapy分布式爬虫

### 第18章 人性化PySpider爬虫框架

# 读书笔记

# 本书评论

## 书评 No.1 
越看越有味道的一本书，就喜欢这种类型的，结构清晰 ^14417636-6W5v4Rz4o
⏱ 2017-12-30 12:05:53
